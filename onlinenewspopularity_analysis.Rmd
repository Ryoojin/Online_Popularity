---
title: "Regression"
author: "Mariia Mohyla"
date: "2023-05-01"
output: pdf_document
---

## Loading the data
```{r}
news = read.csv("~/Desktop/CIS9660/OnlineNewsPopularity.csv")
```

**Primary predictors:**

- Rate of positive words in the content

- Rate of negative words in the content

- Average polarity of positive words

- Average polarity of negative words


**Other control variables:**

- Number of words in the title

- Number of words in the content

- Number of unique words in the content

- Number of images

- Number of videos

- Was the article published on the weekend?

- Is data channel 'Entertainment'?

- Is data channel 'Lifestyle'?

- Is data channel 'Business'?

- Is data channel 'Social Media'?

- Is data channel 'Tech'?

- Is data channel 'World'?


```{r}
library(dplyr)
library(stargazer)
news = news %>%
  select('shares', 'global_rate_positive_words',
           'global_rate_negative_words','avg_positive_polarity',
           'avg_negative_polarity','n_tokens_title',
           'n_tokens_content', 'n_unique_tokens', 'num_imgs',
           'num_videos','is_weekend','data_channel_is_lifestyle',
           'data_channel_is_entertainment','data_channel_is_bus',
           'data_channel_is_socmed',
           'data_channel_is_tech', 'data_channel_is_world')
```

## Changing data types
```{r}
names <- c( 'is_weekend','data_channel_is_lifestyle',
            'data_channel_is_entertainment','data_channel_is_bus',
            'data_channel_is_socmed',
            'data_channel_is_tech', 'data_channel_is_world')
news[,names] <- lapply(news[,names] , factor)
```

## summary statistics
```{r}
summary(news)
```

## Plot missing values in each column and data types
```{r}
library(visdat)
vis_dat(news)
```


```{r}
#install.packages("corrplot")
library(corrplot)
news.cor = cor(news[,1:9])
corrplot(news.cor, method="circle")
```

## Distribution of variables
```{r}
par(mfrow=c(3,3))
boxplot(news[,'shares'], main = c("Boxplot of Shares"), col = 'blue')
boxplot(news[,'global_rate_positive_words'], main = c("Boxplot of Rate of positive words"), col = 'blue')
boxplot(news[,'global_rate_negative_words'], main = paste("Boxplot of Rate of negative words"), col = 'blue')
boxplot(news[,'global_rate_negative_words'], main = paste("Boxplot of Rate of negative words"), col = 'blue')
boxplot(news[,'avg_positive_polarity'], main = paste("Boxplot of Average positive Polarity"), col='blue')
boxplot(news[,'avg_negative_polarity'], main = paste("Boxplot of Average positive Polarity"), col='blue')
```

```{r}
par(mfrow=c(3,3))
boxplot(news[,'n_tokens_title'], main = paste("Boxplot of Number of words in the title"), col='blue')
boxplot(news[,'n_tokens_content'], main = paste("Boxplot of Number of words in the content"), col='blue')
boxplot(news[,'n_unique_tokens'], main = paste("Boxplot of Number of unique words in the content"), col='blue')
boxplot(news[,'num_imgs'], main = paste("Boxplot of Number of images in the content"), col='blue')
boxplot(news[,'num_videos'], main = paste("Boxplot of Number of videos in the content"), col='blue')
```

## Make the histogram
```{r}
library(ggplot2)
news %>%
  ggplot( aes(x=shares)) +
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
  ggtitle("Distribution of shares")
```
 
## Dependent variable is highly skewed we will take log()
```{r}
news_logged = news %>%
  mutate(logged_shares = log(shares))
```


```{r}
library(ggplot2)
news_logged %>%
  ggplot( aes(x=logged_shares)) +
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
  ggtitle("Distribution of shares")
```

## Creating data frame
```{r}
news_logged <- news_logged %>%
    select('logged_shares', 'global_rate_positive_words',
           'global_rate_negative_words','avg_positive_polarity',
           'avg_negative_polarity','n_tokens_title', 'n_unique_tokens',
           'n_tokens_content','num_imgs',
           'num_videos','is_weekend','data_channel_is_lifestyle',
           'data_channel_is_entertainment','data_channel_is_bus',
           'data_channel_is_socmed',
           'data_channel_is_tech', 'data_channel_is_world')
summary(news_logged)
```


## Plot the relationship between priary predictors and log(shares)
```{r}
rate_positive_wrds <- news_logged %>%
  ggplot(aes(x=global_rate_positive_words, y=logged_shares)) +
  geom_point() + geom_smooth(method = 'lm')
rate_negative_wrds <- news_logged %>%
  ggplot(aes(x=global_rate_negative_words, y=logged_shares)) +
  geom_point() + geom_smooth(method = 'lm')
avg_positive_polarity <- news_logged %>%
  ggplot(aes(x=avg_positive_polarity, y=logged_shares)) +
  geom_point() + geom_smooth(method = 'lm')
avg_negative_polarity <- news_logged %>%
  ggplot(aes(x=avg_negative_polarity, y=logged_shares)) +
  geom_point() + geom_smooth(method = 'lm')
```

```{r}
library(cowplot)
plot_grid(rate_positive_wrds, rate_negative_wrds,avg_positive_polarity,
          avg_negative_polarity, labels=c("A", "B", "C","D"), col= 2, nrow = 2)
```

## Multi-linear regression with all data
```{r}
lm.model <- lm(logged_shares ~ ., data = news_logged)
summary(lm.model)
```

## Diagnostic Plots
```{r}
par(mfrow =c(2,2))
plot(lm.model)
```


## Removing influential point and re-running the regression
```{r}
news_logged <- news_logged[-31038,]
```

```{r}
lm.model <- lm(logged_shares ~ ., data = news_logged)
summary(lm.model)
```

The above multiple linear regression model predicts the number of shares for online news articles based on 19 content characteristics such as the sentiment of the article, the length of the article, and the type of article, among others.

By looking at the coefficients in the output, we can observe that some variables have a significant positive impact on the number of shares an article receives. The value of the intercept (the average number of shares when all predictors are zero) is 7.736, which means that, on average, an article is likely to receive about 4,405 shares (since shares are logged, e^7.736 = 4,405) when all other predictors are zero. The variables title polarity, text subjectivity, Number of words in the content, Number of images, Number of videos, whether the article published on the weekend have the highest positive coefficients, which means that when these variables increase, the number of shares an article receives is likely to increase as well.

On the other hand, some variables have a significant negative impact on the number of shares an article receives. For example, the variables Rate of positive words in the content, Rate of negative words in the content, Average polarity of negative words, Average length of the words in the content and all data channels except social media have negative coefficients, which means that when these variables increase, the number of shares an article receives is likely to decrease.

The adjusted R-squared value of 0.07706 (less than 0.3) indicates that the model explains only a small amount of the variance in the number of shares an article receives. This means that there could be other variables beyond the ones included in the model that influence the number of shares.

The F-statistic value is 175.2 and the p-value being very small indicates that the model is statistically significant, but this does not necessarily mean that it is a good fit for the data. The residual standard error of 0.8939 indicates that the average difference between the observed and predicted values of the response variable (i.e., number of shares) around 0.89 when all predictor variables are included in the model.

In conclusion, the model suggests that certain features of news articles such as the sentiment and subjectivity of the content, the number of images, and the number of videos can have a significant effect on the number of shares the article receives. Online portals can use this information to optimize their articles using these predictors and increase their reach. For instance, articles with high levels of subjectivity or those that are published on weekends may be more likely to be shared. Additionally, articles with a positive sentiment and shorter length may also be more likely to be shared.


## Checking the diagnostic plots
```{r}
par(mfrow =c(2,2))
plot(lm.model)
```


## VIF Score to check collinearity
```{r}
library(car)
vif_values <- vif(lm.model)
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "steelblue")
abline(v = 5, lwd = 3, lty = 2)
```
Everything looks within 0 and 5. No collinearity between variables.

## K=5 5-fold cross validation
```{r}
library(boot)
glm.fit=glm(logged_shares ~ ., data=news_logged)
cv.error5=cv.glm(news_logged,glm.fit,K=5)
cv.error5$K
cv.error5$delta
```

## Scatterplot for all significant variables put together at 5% level of significance
```{r}
news_logged[nrow(news_logged) + 1,] <- NA
df1 <- data.frame(
  no_of_shares = news_logged$logged_shares,
  global_rate_negative_words = news$global_rate_negative_words,
  avg_positive_polarity = news$avg_positive_polarity,
  avg_negative_polarity = news$avg_negative_polarity,
  n_unique_tokens = news_logged$n_unique_tokens,
  no_images = news$num_imgs,
  no_videos = news$num_videos
)
pairs(df1)
```


In this scatter plot analysis, we explored the relationships between various predictor variables and the logged number of shares of online news articles using a scatter plot. To do so, we first selected significant variables based on the results of a multi-linear regression analysis. Although the relationship between the logged number of shares and other predictors was not particularly strong, the scatter plot allowed us to identify potential patterns or trends in the data.

Specifically, we observed the relationship between the logged number of shares and several variables, including the global rate of negative words, the average positive polarity, the average negative polarity, the number of unique words, the number of images, and the number of videos. Our scatter plot analysis of the relationship between the logged number of shares and various predictors, including the number of images and videos included in articles, revealed a potential optimal level of image and video usage that maximizes the number of shared articles. Specifically, as the number of images or videos increases, their impact on the number of shared articles decreases, and the scatter plot gradually narrows towards the center, indicating a diminishing return. The narrowing trend provides evidence that exceeding the optimal level of image and video usage may create user burden and hinder sharing.

The scatter plots for the average rate of negative words and the average rate of positive words provide valuable insights into the relationship between these variables and the number of shared articles. In the scatter plot for the average rate of negative words, we observe a similar pattern as with the number of images and videos, where the impact on the number of shared articles gradually decreases as the average rate of negative words increases. However, there is an outlier with a high number of shared articles that deviates from the overall trend. This outlier could represent a news article with highly controversial or provocative content that may generate a significant amount of engagement despite a high rate of negative words.

The scatter plot for the average rate of positive words has a distinct rhombus shape. We observe that news articles with a moderate rate of positive words tend to have a higher number of shares, while articles with either a low or high rate of positive words have a lower number of shares. This suggests that articles with a moderate amount of positive language may be more appealing to readers, while articles that are either overly positive or overly negative may be less engaging. 

These results highlight the importance of judiciously using images and videos to increase the likelihood of sharing, and emphasize the need for more effective content strategies that balance the potential benefits of image and video use with the outcomes of sharing. By informing the development of effective content strategies, this analysis provides valuable insights that can improve the likelihood of the virality for online news articles.


## Polybinomial Analysis
```{r}
library(dplyr)
set.seed(5)
poly_model <- glm(logged_shares ~ poly(global_rate_positive_words,3) + poly(global_rate_negative_words, 3) +
                    poly(avg_negative_polarity, 3) + poly(avg_positive_polarity, 3) + poly(n_tokens_title, 3) +
                    poly(n_unique_tokens, 3) + poly(n_tokens_content,3) + poly(num_imgs,3) + poly(num_videos,3) +
                    is_weekend + data_channel_is_lifestyle + data_channel_is_socmed +
                    data_channel_is_entertainment + data_channel_is_bus + data_channel_is_tech + data_channel_is_world, data=news_logged)

summary(poly_model)

```

The goal of this analysis was to identify the factors that contribute to the number of shares that online news articles receive on social media. To achieve this, a multiple linear regression model was developed using a dataset of news articles and their associated features. The model included four polynomial terms for the numerical features: num_imgs, global_rate_negative_words, avg_negative_polarity, and n_unique_tokens, as well as six binary features indicating whether the article was published on a certain type of news channel (data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_tech, data_channel_is_world, and is_weekend).

The regression analysis produced coefficients for each of the features in the model. The results show that num_imgs has a significant positive effect on the number of shares, with the coefficient increasing for the first two polynomial terms (poly(num_imgs, 4)1 and poly(num_imgs, 4)2), and decreasing for the third and fourth polynomial terms (poly(num_imgs, 4)3 and poly(num_imgs, 4)4). This suggests that including a moderate number of images can increase the number of shares, but too many or too few images can have a negative effect.

In terms of the other polynomial features, global_rate_negative_words and avg_negative_polarity did not have a significant effect on the number of shares, while n_unique_tokens had a significant positive effect. However, the effect size was small and only significant for the second and third polynomial terms (poly(n_unique_tokens, 4)2 and poly(n_unique_tokens, 4)3).

The binary features in the model were found to have significant effects on the number of shares. Specifically, articles published on lifestyle, entertainment, and world news channels had a negative effect, while articles published on bus and tech news channels had a smaller negative effect. Articles published on weekends had a significant positive effect.

Overall, the regression model had a low R-squared value of 0.075, indicating that the features included in the model explain only a small portion of the variation in the number of shares. While the model can be improved, the results suggest that publishers can increase the number of shares by including a moderate number of images and publishing articles on weekdays, and by avoiding publishing articles on lifestyle, entertainment, and world news channels.


## K=5 5-fold cross validation_Polybinomial model
```{r}
library(boot)
# Perform 5-fold cross-validation
cv <- cv.glm(news_logged, poly_model,K = 5)

cv$K
cv$delta
```


## Random Forest
```{r}
library(randomForest)
set.seed(1)
rf.news=randomForest(logged_shares~., data=news_logged, mtry=16/3,importance=TRUE)
```

```{r}
importance(rf.news)
varImpPlot(rf.news)
```

# Classification
## Creating a factor whether an article is popular or not
```{r}
popular=as.factor(ifelse(news_logged$logged_shares<=7.3,'No','Yes')) # 7.244 is the median log of shares
news_logged_class=data.frame(news_logged,popular)
news_logged_class =  news_logged_class %>% 
  select(-logged_shares)
summary(news_logged_class) 
```

## Create functions to calculate precision, recall and accuracy rate
```{r}
recall = function(cm) {
return(cm[2,2]/(cm[2,2]+cm[1,2]))
}
accuracy = function(cm) {
return((cm[1,1]+cm[2,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2]))
}
precision = function(cm) {
return((cm[2,2])/(cm[2,2]+cm[2,1]))
}
```

## Single classification Tree
```{r}
set.seed(2)
news.train = sample(1:nrow(news_logged_class), nrow(news_logged_class)/2)
news.test = news_logged_class[-news.train,]
popular.test = news_logged_class$popular[-news.train]
```

```{r}
library(tree)
tree.news=tree(popular~.,news_logged_class, subset=news.train)
summary(tree.news)
```

```{r}
plot(tree.news)
text(tree.news,pretty=0)
```

## tree prediction
```{r}
tree.pred=predict(tree.news,news.test,type="class")
```

## create a confusion matrix
```{r}
single.tree.cm = table(tree.pred, popular.test)
single.tree.cm
```

## accuracy rate
```{r}
accuracy.rate1 = mean(tree.pred==popular.test)
accuracy.rate1
```

## precision rate
```{r}
pr.rate1 = precision(single.tree.cm)
pr.rate1
```

## recall rate
```{r}
recall.rate1 = recall(single.tree.cm)
```

## Logistic regression
```{r}
set.seed(5)
glm.fits=glm(popular~.,data=news_logged_class,family=binomial, subset=news.train)
```

```{r}
summary(glm.fits)
```

```{r}
glm.probs=predict(glm.fits,news.test,type="response")
```

```{r}
length(glm.probs)
```

```{r}
glm.pred=rep('No',19822)
```

```{r}
glm.pred[glm.probs>.5]="Yes"
```

## confusion matrix
```{r}
logistic.reg.cm = table(glm.pred,popular.test)
logistic.reg.cm
```

## accuracy rate
```{r}
accuracy.rate2 = mean(glm.pred==popular.test)
accuracy.rate2
```

## precision rate
```{r}
pr.rate2 = precision(logistic.reg.cm)
pr.rate2
```

The cost of false positive is high we want to keep high precision rate.

## recall rate
```{r}
recall.rate2 = recall(logistic.reg.cm)
recall.rate2
```

## Random Forest
```{r}
set.seed(1)
rf.news_class=randomForest(popular~.,data=news_logged_class,subset=news.train,mtry=sqrt(16),importance=TRUE)
yhat.rf = predict(rf.news_class,newdata=news.test)
```

## confusion matrix
```{r}
rf.cm = table(yhat.rf,popular.test)
rf.cm
```
## accuracy rate
```{r}
accuracy.rate3 = mean(yhat.rf==popular.test)
accuracy.rate3
```

## precision rate
```{r}
pr.rate3 = precision(rf.cm)
pr.rate3
```

## recall rate
```{r}
recall.rate3 = recall(rf.cm)
recall.rate3
```

```{r}
importance(rf.news)
varImpPlot(rf.news)
```

```{r}
(comparing_metrics = data.frame(Models = c('Single Tree','Logistic Regression','Random Forest'), 
                                Accuracy = c(accuracy.rate1, accuracy.rate2, accuracy.rate3),
                                Precision = c(pr.rate1, pr.rate2, pr.rate3),
                                Recall = c(recall.rate1,recall.rate2, recall.rate3)))
```





